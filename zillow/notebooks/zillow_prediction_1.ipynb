{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import xgboost\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop_2016 = pd.read_csv('/Users/fangjie/Downloads/properties_2016.csv')\n",
    "df_prop_2017 = pd.read_csv('/Users/fangjie/Downloads/properties_2017.csv')\n",
    "df_prop = df_prop_2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop_2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File /Users/fangjie/Downloads/train_2017_v2.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-86a21e5ba1e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_trans_2016\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/fangjie/Downloads/train_2016_v2.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_trans_2017\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/fangjie/Downloads/train_2017_v2.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_trans_2016\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_trans_2017\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fangjie/Workspace/virtual-environments/new/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fangjie/Workspace/virtual-environments/new/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fangjie/Workspace/virtual-environments/new/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fangjie/Workspace/virtual-environments/new/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fangjie/Workspace/virtual-environments/new/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File /Users/fangjie/Downloads/train_2017_v2.csv does not exist"
     ]
    }
   ],
   "source": [
    "df_trans_2016 = pd.read_csv('/Users/fangjie/Downloads/train_2016_v2.csv', parse_dates=[2])\n",
    "df_trans_2017 = pd.read_csv('/Users/fangjie/Downloads/train_2017_v2.csv', parse_dates=[2])\n",
    "df_trans = pd.concat((df_trans_2016, df_trans_2017), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_2016.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop_2016.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans.transactiondate.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parcelid                          int64\n",
       "airconditioningtypeid           float64\n",
       "architecturalstyletypeid        float64\n",
       "basementsqft                    float64\n",
       "bathroomcnt                     float64\n",
       "bedroomcnt                      float64\n",
       "buildingclasstypeid             float64\n",
       "buildingqualitytypeid           float64\n",
       "calculatedbathnbr               float64\n",
       "decktypeid                      float64\n",
       "finishedfloor1squarefeet        float64\n",
       "calculatedfinishedsquarefeet    float64\n",
       "finishedsquarefeet12            float64\n",
       "finishedsquarefeet13            float64\n",
       "finishedsquarefeet15            float64\n",
       "finishedsquarefeet50            float64\n",
       "finishedsquarefeet6             float64\n",
       "fips                            float64\n",
       "fireplacecnt                    float64\n",
       "fullbathcnt                     float64\n",
       "garagecarcnt                    float64\n",
       "garagetotalsqft                 float64\n",
       "hashottuborspa                   object\n",
       "heatingorsystemtypeid           float64\n",
       "latitude                        float64\n",
       "longitude                       float64\n",
       "lotsizesquarefeet               float64\n",
       "poolcnt                         float64\n",
       "poolsizesum                     float64\n",
       "pooltypeid10                    float64\n",
       "pooltypeid2                     float64\n",
       "pooltypeid7                     float64\n",
       "propertycountylandusecode        object\n",
       "propertylandusetypeid           float64\n",
       "propertyzoningdesc               object\n",
       "rawcensustractandblock          float64\n",
       "regionidcity                    float64\n",
       "regionidcounty                  float64\n",
       "regionidneighborhood            float64\n",
       "regionidzip                     float64\n",
       "roomcnt                         float64\n",
       "storytypeid                     float64\n",
       "threequarterbathnbr             float64\n",
       "typeconstructiontypeid          float64\n",
       "unitcnt                         float64\n",
       "yardbuildingsqft17              float64\n",
       "yardbuildingsqft26              float64\n",
       "yearbuilt                       float64\n",
       "numberofstories                 float64\n",
       "fireplaceflag                    object\n",
       "structuretaxvaluedollarcnt      float64\n",
       "taxvaluedollarcnt               float64\n",
       "assessmentyear                  float64\n",
       "landtaxvaluedollarcnt           float64\n",
       "taxamount                       float64\n",
       "taxdelinquencyflag               object\n",
       "taxdelinquencyyear              float64\n",
       "censustractandblock             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prop_2016.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'parcelid', u'airconditioningtypeid', u'architecturalstyletypeid',\n",
       "       u'basementsqft', u'bathroomcnt', u'bedroomcnt', u'buildingclasstypeid',\n",
       "       u'buildingqualitytypeid', u'calculatedbathnbr', u'decktypeid',\n",
       "       u'finishedfloor1squarefeet', u'calculatedfinishedsquarefeet',\n",
       "       u'finishedsquarefeet12', u'finishedsquarefeet13',\n",
       "       u'finishedsquarefeet15', u'finishedsquarefeet50',\n",
       "       u'finishedsquarefeet6', u'fips', u'fireplacecnt', u'fullbathcnt',\n",
       "       u'garagecarcnt', u'garagetotalsqft', u'hashottuborspa',\n",
       "       u'heatingorsystemtypeid', u'latitude', u'longitude',\n",
       "       u'lotsizesquarefeet', u'poolcnt', u'poolsizesum', u'pooltypeid10',\n",
       "       u'pooltypeid2', u'pooltypeid7', u'propertycountylandusecode',\n",
       "       u'propertylandusetypeid', u'propertyzoningdesc',\n",
       "       u'rawcensustractandblock', u'regionidcity', u'regionidcounty',\n",
       "       u'regionidneighborhood', u'regionidzip', u'roomcnt', u'storytypeid',\n",
       "       u'threequarterbathnbr', u'typeconstructiontypeid', u'unitcnt',\n",
       "       u'yardbuildingsqft17', u'yardbuildingsqft26', u'yearbuilt',\n",
       "       u'numberofstories', u'fireplaceflag', u'structuretaxvaluedollarcnt',\n",
       "       u'taxvaluedollarcnt', u'assessmentyear', u'landtaxvaluedollarcnt',\n",
       "       u'taxamount', u'taxdelinquencyflag', u'taxdelinquencyyear',\n",
       "       u'censustractandblock'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prop_2016.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # check duplicates for each parcelID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prop_2016.parcelid.unique().shape[0] == df_prop_2016.parcelid.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put just treat houses that are sold multiple times as different hosues for simplication purpose\n",
    "(df_trans.parcelid.value_counts() > 1).sum() * 1.0 / df_trans.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(left=df_prop_2016, right=df_trans_2016, on='parcelid')\n",
    "df_pred = pd.merge(left=df_prop, right=df_trans.drop_duplicates(subset='parcelid',keep='first'), on='parcelid', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.censustractandblock.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    61110022.003\n",
       "1    61110015.031\n",
       "2    61110007.011\n",
       "3    61110008.002\n",
       "4    61110014.021\n",
       "Name: rawcensustractandblock, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rawcensustractandblock.head().astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fields = set(df.columns)\n",
    "\n",
    "# these are fields that are used to identify fields\n",
    "identifiers = set(['transactiondate', 'parcelid'])\n",
    "\n",
    "# log error that we want to model\n",
    "label = set(['logerror'])\n",
    "\n",
    "# the following are categorical features\n",
    "feat_objects = set(df.columns[df.dtypes==object])\n",
    "#  ['taxdelinquencyflag',\n",
    "#  'propertycountylandusecode',\n",
    "#  'propertyzoningdesc',\n",
    "#  'fireplaceflag',\n",
    "#  'hashottuborspa']\n",
    "\n",
    "# the following are numerical features that should be treated as categorical features\n",
    "feat_numeric_to_categorical = set([\n",
    "    'airconditioningtypeid',\n",
    "    'architecturalstyletypeid',\n",
    "    'buildingqualitytypeid',\n",
    "    'buildingclasstypeid',\n",
    "    'decktypeid',\n",
    "    'fips',\n",
    "    'heatingorsystemtypeid',\n",
    "    'propertylandusetypeid',\n",
    "    'regionidcounty',\n",
    "    'regionidcity',\n",
    "    'regionidzip',\n",
    "    'regionidneighborhood',\n",
    "    'storytypeid',\n",
    "    'typeconstructiontypeid',\n",
    "    'yearbuilt',\n",
    "    'assessmentyear',\n",
    "    'taxdelinquencyyear'\n",
    "])\n",
    "\n",
    "\n",
    "# the rest are numeric features\n",
    "feat_numeric = set([\n",
    "    'basementsqft',\n",
    "    'bathroomcnt',\n",
    "    'bedroomcnt',\n",
    "    'calculatedbathnbr',\n",
    "    'threequarterbathnbr',\n",
    "    'finishedfloor1squarefeet',\n",
    "    'calculatedfinishedsquarefeet',\n",
    "    'finishedsquarefeet6',\n",
    "    'finishedsquarefeet12',\n",
    "    'finishedsquarefeet13',\n",
    "    'finishedsquarefeet15',\n",
    "    'finishedsquarefeet50',\n",
    "    'fireplacecnt',\n",
    "    'fullbathcnt',\n",
    "    'garagecarcnt',\n",
    "    'garagetotalsqft',\n",
    "    'hashottuborspa',\n",
    "    'lotsizesquarefeet',\n",
    "    'numberofstories',\n",
    "    'poolcnt',\n",
    "    'poolsizesum',\n",
    "    'pooltypeid10',\n",
    "    'pooltypeid2',\n",
    "    'pooltypeid7',\n",
    "    'roomcnt',\n",
    "    'unitcnt',\n",
    "    'yardbuildingsqft17',\n",
    "    'yardbuildingsqft26',\n",
    "    'taxvaluedollarcnt',\n",
    "    'structuretaxvaluedollarcnt',\n",
    "    'landtaxvaluedollarcnt',\n",
    "    'taxamount'\n",
    "])\n",
    "\n",
    "# fields that are thrown away for now\n",
    "feat_for_consideration_later = set([\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'rawcensustractandblock',\n",
    "    'censustractandblock',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eats_categorical = feat_numeric_to_categorical | feat_objects\n",
    "feat_numeric = feat_numeric\n",
    "feat = feat_categorical | feat_numeric\n",
    "\n",
    "feat_categorical = list(feat_categorical)\n",
    "feat_numeric = list(feat_numeric)\n",
    "feat = list(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #convert categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to impute all these with a `missing` value first\n",
    "df[feat_categorical] = df[feat_categorical].fillna('--unknown--')\n",
    "df_pred[feat_categorical] = df_pred[feat_categorical].fillna('--unknown--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use label encoder to encode all categorical variables\n",
    "le = LabelEncoder()\n",
    "for feat in feat_categorical:\n",
    "    df_pred[[feat]] = df_pred[[feat]].apply(le.fit_transform)\n",
    "    df[[feat]] = df[[feat]].apply(le.transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## impute missing values for numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp =  Imputer(missing_values=np.nan,strategy=\"median\",axis=0)\n",
    "\n",
    "df_pred[feat_numeric] = imp.fit_transform(df_pred[feat_numeric])\n",
    "df[feat_numeric] = imp.transform(df[feat_numeric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = df.transactiondate < '2017-06-01'\n",
    "# mask_test_oct = (df.transactiondate >= '2016-10-01') & (df.transactiondate < '2016-11-01')\n",
    "# mask_test_nov = (df.transactiondate >= '2016-11-01') & (df.transactiondate < '2016-12-01')\n",
    "# mask_test_dec = (df.transactiondate >= '2016-12-01') & (df.transactiondate < '2017-01-01')\n",
    "# mask_test = mask_test_oct | mask_test_nov | mask_test_dec\n",
    "mask_test = (df.transactiondate >= '2017-06-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[mask_train, feats].astype(float).values\n",
    "X_test = df.loc[mask_test, feats].astype(float).values\n",
    "\n",
    "y_train = np.array(df.loc[mask_train, 'logerror'].tolist())\n",
    "y_test = np.array(df.loc[mask_test, 'logerror'].tolist())\n",
    "\n",
    "data = (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "pickle.dump(data, open('../tmp/training_and_testing_data_2017.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train a XGboost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBRegressor(n_estimators=200, \n",
    "                           #colsample_bytree=1, \n",
    "                           max_depth=3, \n",
    "                           #reg_alpha=0.001,\n",
    "                           objective='reg:linear'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_pred).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_score(y_pred=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_pred=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #generate final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(np.concatenate((X_train, X_test), axis=0),np.concatenate((y_train, y_test), axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputed missing values previously\n",
    "y_pred = xgb.predict((df_pred[feats].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume all 6 months have the same prediction results.\n",
    "df_submission = df_pred[['parcelid','logerror']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission['pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coalesce actual log error with prediction results\n",
    "\n",
    "df_submission['201610'] = df_submission['logerror'].combine_first(df_submission['pred'])\n",
    "df_submission['201611'] = df_submission['logerror'].combine_first(df_submission['pred'])\n",
    "df_submission['201612'] = df_submission['logerror'].combine_first(df_submission['pred'])\n",
    "\n",
    "df_submission['201710'] = df_submission['logerror'].combine_first(df_submission['pred'])\n",
    "df_submission['201711'] = df_submission['logerror'].combine_first(df_submission['pred'])\n",
    "df_submission['201712'] = df_submission['logerror'].combine_first(df_submission['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_submission['logerror']\n",
    "del df_submission['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.shape[0] == 2985217\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('Users/fangjie/Downloads/submission_20171003_2017.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
